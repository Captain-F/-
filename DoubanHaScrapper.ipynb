{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "from itertools import cycle\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取的数据包括：\n",
    "#1.标题 2.作者昵称 3.作者id 4.回应数 \n",
    "#5.时间 6.帖子内容 7. 帖子url 8.回帖内容 9.帖子中包含图片的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADERS_LIST = [\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; x64; fr; rv:1.9.2.13) Gecko/20101203 Firebird/3.6.13',\n",
    "    'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows NT 5.2; RW; rv:7.0a1) Gecko/20091211 SeaMonkey/9.23a1pre'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#随机更换代理，是反爬的trick之一，但是在实践过程中，并不work；\n",
    "#更换header也是，但是从爬取的实践经验来看，time.sleep()的设置是非常重要的；\n",
    "#sleep的时间设置长些，也就意味着访问url的间隔时间会长些，这样可以有效避免反爬；\n",
    "def get_proxies():\n",
    "    response = requests.get(PROXY_URL)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    table = soup.find('table',id='proxylisttable')\n",
    "    list_tr = table.find_all('tr')\n",
    "    list_td = [elem.find_all('td') for elem in list_tr]\n",
    "    list_td = list(filter(None, list_td))\n",
    "    list_ip = [elem[0].text for elem in list_td]\n",
    "    list_ports = [elem[1].text for elem in list_td]\n",
    "    list_proxies = [':'.join(elem) for elem in list(zip(list_ip, list_ports))]\n",
    "    return list_proxies  \n",
    "\n",
    "def get_data(num, HEADER):\n",
    "    basic_url = 'https://www.douban.com/group/638298/discussion?start=%d'%(num*25)\n",
    "    #basic_url = 'https://www.douban.com/group/638298/discussion?start=25'\n",
    "    response = requests.get(basic_url, headers = HEADER, timeout= 60)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    #获取帖子中的title,name,response,time;\n",
    "    #filtered_data = ['title','name','response','time']\n",
    "    data = soup.find_all('td')\n",
    "    filtered_data = [str(i.text.strip()) for i in data][5:]#['','']\n",
    "        \n",
    "    #获取帖子中的topic's url及发帖人id；\n",
    "    topic_people_urls = [url.get('href') for url in soup.find_all('a')]\n",
    "    topic_url = []\n",
    "    people_url = []\n",
    "    for topic_people_url in topic_people_urls:\n",
    "        topic_people_url_ = topic_people_url.split('/')\n",
    "        #获得topic_url;\n",
    "        if len(topic_people_url_) < 3:\n",
    "            continue\n",
    "        if topic_people_url_[-3] == 'topic':\n",
    "            topic_url.append(topic_people_url)\n",
    "        #获得people_url;\n",
    "        elif topic_people_url_[-3] == 'people':\n",
    "            people_url.append(topic_people_url)\n",
    "             \n",
    "    return filtered_data, topic_url, people_url\n",
    "\n",
    "def get_content(topic_urls, HEADER):\n",
    "    \n",
    "    docs = []\n",
    "    count_ = []\n",
    "    reply_content = []\n",
    "    img_urls_ = []\n",
    "    for url in topic_urls:\n",
    "        response = requests.get(url, headers = HEADER, timeout= 60)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        #获取帖子内容；\n",
    "        titles = soup.find_all('div','topic-doc')\n",
    "        topic_docs = [str(i.text.strip()) for i in titles]\n",
    "        try:\n",
    "            topic_docs_ = [i.split('\\n\\n\\n\\n\\n')[1].strip() for i in topic_docs]\n",
    "        except:\n",
    "            topic_docs_ = ['空']\n",
    "        docs.extend(topic_docs_)\n",
    "        \n",
    "        #获取帖子中含有图片的数量；\n",
    "        img_urls = [i.get('src') for i in soup.find_all('img')]\n",
    "        img_urls_.append(img_urls)\n",
    "        \n",
    "        #获取回帖内容；\n",
    "        reply = [i.text.strip() for i in soup.find_all('div', 'reply-doc content')]\n",
    "        reply_ = [i.split('\\n\\n')[1].strip() for i in reply]\n",
    "        reply_content.append(reply_)\n",
    "        time.sleep(3)\n",
    "        \n",
    "    #获取帖子所发图片的数量\n",
    "    for i in img_urls_:\n",
    "        count = 0\n",
    "        for j in i:\n",
    "            if j.split('/')[-2] == 'public':\n",
    "                count = count + 1\n",
    "        count_.append(count)\n",
    "\n",
    "    return docs, reply_content, count_\n",
    "\n",
    "def get_page_nums():\n",
    "    basic_url = 'https://www.douban.com/group/638298/discussion?start=0'\n",
    "    response = requests.get(basic_url, headers = HEADER)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    nums = str(soup.find('span', 'thispage')).split('\"')[-2]\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "哈哈哈哈哈哈哈\n",
      "126.90873217582703\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "nums = get_page_nums()\n",
    "\n",
    "#for i in range(int(nums)+1):\n",
    "for i in range(1):\n",
    "    s_time = time.time()\n",
    "    print(i)\n",
    "    HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "    \n",
    "    try:\n",
    "        filtered_data, topic_url, people_url = get_data(i, HEADER)\n",
    "        time.sleep(3)\n",
    "        docs, reply_content, count_= get_content(topic_url, HEADER)\n",
    "    except:\n",
    "        filtered_data, topic_url, people_url = get_data(i, HEADER)\n",
    "        time.sleep(3)\n",
    "        docs, reply_content, count_= get_content(topic_url, HEADER)\n",
    "        \n",
    "    titles = [filtered_data[i] for i in range(0, len(filtered_data), 4)]\n",
    "    names = [filtered_data[i] for i in range(1, len(filtered_data), 4)]\n",
    "    responses = [filtered_data[i] for i in range(2, len(filtered_data), 4)]\n",
    "    times = [filtered_data[i] for i in range(3, len(filtered_data), 4)]\n",
    "    \n",
    "    csv = pd.DataFrame({'title':titles, 'name':names, 'id':people_url,\n",
    "                        'response_nums':responses, 'time':times, 'docs':docs,\n",
    "                        'reply_content':reply_content, 'img_nums':count_, \n",
    "                       'topic_url':topic_url})\n",
    "    csv.to_csv(r'hhhhhhhh.csv', mode= 'a', header= False)\n",
    "    time.sleep(5)\n",
    "    print('哈哈哈哈哈哈哈')\n",
    "    e_time = time.time()\n",
    "    print(e_time-s_time)\n",
    "    print('===========')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
